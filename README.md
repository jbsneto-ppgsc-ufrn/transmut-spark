# TRANSMUT-Spark 

## Transformation Mutation for Apache Spark

TRANSMUT-Spark is a [sbt](https://www.scala-sbt.org/index.html) plugin for mutation testing of Apache Spark programs in Scala. It applies transformation mutation to insert faults in the set of transformations called in a Spark program. TRANSMUT-Spark runs the entire mutation testing process by generating the mutants, running the tests for each mutant and analyzing the test results to identify killed and survived mutants, generating reports with the results and metrics. The TRANSMUT-Spark plugin is based on the [Stryker4s](https://github.com/stryker-mutator/stryker4s) sbt plugin.

### Requirements

* sbt 1.3+
* Scala 2.12.x
* SemanticDB enabled (`semanticdbEnabled := true`)
* Spark programs using only the RDD (Core) API

### Add Plugin

The TRANSMUT-Spark plugin has not yet been published to a remote repository, to use it you must clone this project and publish the plugin locally with `sbt publishLocal` inside the TRANSMUT-Spark project folder. After that, the plugin is able to be added to local projects.

To install the TRANSMUT-Spark plugin in a project, add the following line to your `project/plugins.sbt`:

```scala
addSbtPlugin("br.ufrn.dimap.forall" % "sbt-transmut" % "0.1-SNAPSHOT")
```

### Usage

To use TRANSMUT-Spark, enter `transmut` in the sbt console or `sbt transmut` in the terminal inside the folder of the project with the programs to be mutated. It triggers the execution of TRANSMUT-Spark that will look for the `transmut.conf` configuration file in the root of the project and run the entire process. 

Additionally, there is also the `transmutAlive` command that can be executed after a first execution of `transmut`. The `transmutAlive` considers the results of the last execution of TRANSMUT-Spark to execute only the surviving mutants of the last execution again. This command is useful to analyze only the surviving mutants without having to run all the generated mutants. To run `transmutAlive`, the options `sources`, `programs` and `mutation-operators` in the configurations must be the same as in the configurations of the last run.

### Outputs

The successful execution of TRANSMUT-Spark will generate the folder `transmut-\$datetime` inside `target/` (or other folder defined in `transmut-base-dir` in configurations) containing the following content:

* `mutants/`
	* Folder with the meta-mutant sources generated by TRANSMUT-Spark. A meta-mutant is a source that agregates all mutants generated for this source in just a single code. To create this meta-mutant, we apply the [Mutation Switching](https://stryker-mutator.io/blog/2018-10-6/mutation-switching) technique used by [Stryker4s](https://stryker-mutator.io/stryker4s/).
* `mutated-src/`
	* Folder with a copy of the project's Scala source folder (`src/main/scala/` or other folder defined by `src-dir` in configurations) and modified sources. To not modify the original source codes, we copy the folder and mutate the sources inside the copy. This folder contains the modified sources and the other sources that were not modified in the process. To see only the modified sources, look at the source codes in the `mutants` folder.
* `reports/`
	* Folder with the reports generated by TRANSMUT-Spark.
* `reports/html/`
	* Folder with the HTML reports generated by TRANSMUT-Spark.
* `reports/json/`
	* Folder with the JSON reports generated by TRANSMUT-Spark.
* `transmut.conf`
	* Copy of the configuration file that was used in this run.

### Configurations

Configurations of TRANSMUT-Spark are set in the `transmut.conf` file in the root of the project. The file is in the HOCON format and all configuration options should be in the "transmut" namespace. For example:

```
transmut {
    sources: [ "WordCount.scala" ],
    programs: [ "wordCount" ]
}
```

#### Configuration Options:
* `sources`
	* **Description:** list of file names of the program source codes (Scala source codes with the programs to be mutated). Only the file name must be entered, not the full path.
	* **Mandatory:** Yes
	* **Example:** `sources: [ "WordCount.scala" ]`
* `programs`
	* **Description:** list of programs (methods) to be mutated. For TRANSMUT-Spark, a Spark program must be encapsulated in a method to be mutated. The programs in the list must exist in one of the sources. Only the methods in the list are mutated, other methods and statements of the sources remain unchanged.
	* **Mandatory:** Yes
	* **Example:** `programs: [ "wordCount" ]`
	* **Warning:** if the programs in the list are not identified in any source, the execution will fail.
* `mutation-operators`
	* **Description:** list of mutation operators to be applied in the mutation testing process. Only the mutation operators in the list are applied to generate mutants.
	* **Mandatory:** No
	* **Default:** `[ "ALL" ]`
	* **Example:** `mutation-operators: [ "DTI", "ATR", "JTR" ]`
* `equivalent-mutants`
	* **Description:** list of equivalent mutants IDs. This list must be updated after a first run of the tool and an analysis of the survived mutants if they are identified as equivalent. If the list of programs, original program codes and/or list of mutation operators are not changed, different executions of TRANSMUT-Spark will always generate the same mutants and with the same IDs. Mutants in this list are not executed in the tests and are marked as equivalent in the reports.
	* **Mandatory:** No
	* **Default:** ` [] `
	* **Example:** `equivalent-mutants: [ 3, 5, 6 ]`
	* **Warning:** if the list of programs, original program codes and/or list of mutation operators are changed after a run, the generated mutants and IDs may differ from those generated in the previous run in a new run.
* `test-only`
	* **Description:** list of test classes to be executed. If the list is empty, all tests of the project are executed. The full name of the test classes must be entered (package + class name).
	* **Mandatory:** No
	* **Default:** ` [] `
	* **Example:** `test-only: [ "examples.WordCountTest" ]`
	* **Warning:** run all tests of the project (in case of `test-only` is empty) can be a slow process on big projects or projects with many other tests that are not related with the programs to be mutated. In this case, specifying the test classes to be executed is recommended to speed up the process.
* `src-dir`
	* **Description:** The directory containing the sources of the programs to be mutated.
	* **Mandatory:** No
	* **Default:** `'src/main/scala/'` (scalaSource from sbt)
	* **Example:** `src-dir: 'other-src-folder/main/scala/'`
* `semanticdb-dir`
	* **Description:** The directory containing the semanticdb specifications generated by the SemanticDB compile plugin. TRANSMUT-Spark is dependent on SemanticDB compile plugin, so SemanticDB must be enabled in the project settings (`semanticdbEnabled := true` in the `build.sbt` file)
	* **Mandatory:** No
	* **Default:** `'target/scala-2.12/meta/'` (semanticdbTargetRoot from sbt)
	* **Example:** `semanticdb-dir: 'target/scala-2.12/other-meta/'`
	* **Warning:** if the SemanticDB compile plugin is not enabled in the project settings (`semanticdbEnabled := true` in the `build.sbt` file), the execution will fail.
* `transmut-base-dir`
	* **Description:** The directory where the TRANSMUT-Spark folder containing the sources and reports will be created. The folder generated has the name "transmut-\$datetime" where \$datetime has the date and time the process was started in the format "yyyyMMddHHmmss".
	* **Mandatory:** No
	* **Default:** `'target/'`
	* **Example:** `transmut-base-dir: 'other-transmut-base-folder/'`


### Mutation Operators

List of mutation operators supported by TRANSMUT-Spark.

| **Operator** | **Description**                    |
|----------|----------------------------------------|
| UTS      | Unary Transformation Swap              |
| BTS      | Binary Transformation Swap             |
| UTR      | Unary Transformation Replacement       |
| BTR      | Binary Transformation Replacement      |
| UTD      | Unary Transformation Deletion          |
| MTR      | Mapping Transformation Replacement     |
| FTD      | Filter Transformation Deletion         |
| STR      | Set Transformation Replacement         |
| DTD      | Distinct Transformation Deletion       |
| DTI      | Distinct Transformation Insertion      |
| ATR      | Aggregation Transformation Replacement |
| JTR      | Join Transformation Replacement        |
| OTD      | Order Transformation Deletion          |

In the list of mutation operators in configurations (`mutation-operators`), use "ALL" as an alias to apply all mutation operators.


### Restrictions
* References (parameters, variables and values) must have unique names;
* Programs to be mutated must be encapsulated in methods;
* All RDDs must have their own reference (must be declared as a parameter, variable or value);
* Only one transformation must be called in a statement:
	* For example: `val rdd2 = rdd.filter( (a: String) => !a.isEmpty )`
* Anonymous (lambda) functions must have their input parameters explicitly typed:
	* Incorrect: `rdd.map( a => a * a )`
	* Correct: `rdd.map( (a: Int) => a * a )`

	
Example of a traditional Spark program that is not in the format supported by TRANSMUT-Spark:

```scala
package examples

import org.apache.spark._

object WordCount {

	def main(args: Array[String]): Unit = {
		val conf = new SparkConf()
    	conf.setAppName("Word-Count")
    	val sc = new SparkContext(conf)
		val textFile = sc.textFile("hdfs://...")
		val counts = textFile.flatMap(line => line.split(" "))
       					.map(word => (word, 1))
       					.reduceByKey(_ + _)
		counts.saveAsTextFile("hdfs://...")
	}
  
}
```

Example of the same program in a version supported by TRANSMUT-Spark:

```scala
package examples

import org.apache.spark._
import org.apache.spark.rdd.RDD

object WordCount {

	def wordCount(input: RDD[String]) = {
		val words = input.flatMap( (line: String) => line.split(" ") )
		val pairs = words.map( (word: String) => (word, 1) )
		val counts = pairs.reduceByKey( (a: Int, b: Int) => a + b )
		counts
	}
	
	def main(args: Array[String]): Unit = {
		val conf = new SparkConf()
    	conf.setAppName("Word-Count")
    	val sc = new SparkContext(conf)
		val textFile = sc.textFile("hdfs://...")
		val results = wordCount(textFile)
		results.saveAsTextFile("hdfs://...")
	}
  
}
```
	
Considering the example above, only the method `wordCount` should be included as a program to be mutated in the TRANSMUT-Spark configurations. Open the [example project](transmut-spark-example-project/) to see more examples of programs in the format supported by TRANSMUT-Spark and to run the plugin.

### Supported Transformations

List of RDD transformations that can be mutated by TRANSMUT-Spark. Transformations that are not in this list can be in the program, but will not be modified in the process.

| Transformation                 | Interface                                                                                                   |
|--------------------------------|-------------------------------------------------------------------------------------------------------------|
| map                            | `map[U](f: (T) ⇒ U): RDD[U]`                                                                                |
| flatMap                        | `flatMap[U](f: (T) ⇒ TraversableOnce[U]): RDD[U]`                                                           |
| filter                         | `filter(f: (T) ⇒ Boolean): RDD[T]`                                                                          |
| distinct                       | `distinct(): RDD[T]`                                                                                        |
| sortBy                         | `sortBy[K](f: (T) ⇒ K, ascending: Boolean = true): RDD[T]`                                                  |
| sortByKey                      | `sortByKey(ascending: Boolean = true): RDD[(K, V)]`                                                         |
| union                          | `union(other: RDD[T]): RDD[T]`                                                                              |
| intersection                   | `intersection(other: RDD[T]): RDD[T]`                                                                       |
| subtract                       | `subtract(other: RDD[T]): RDD[T]`                                                                           |
| join                           | `join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]`                                                             |
| leftOuterJoin                  | `leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]`                                            |
| rightOuterJoin                 | `rightOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (Option[V], W))]`                                           |
| fullOuterJoin                  | `fullOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (Option[V], Option[W]))]`                                    |
| reduceByKey                    | `reduceByKey(func: (V, V) ⇒ V): RDD[(K, V)]`                                                                |
| combineByKey* (mergeCombiners) | `combineByKey[C](createCombiner: (V) ⇒ C, mergeValue: (C, V) ⇒ C, mergeCombiners: (C, C) ⇒ C): RDD[(K, C)]` |

<!--| aggregateByKey* (combOp) | `aggregateByKey[U](zeroValue: U)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U): RDD[(K, U)]` |
-->
\* \- Only the parameter in parentheses is mutated, the rest remains the same.